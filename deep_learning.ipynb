{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8b1961",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc6a214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchviz import make_dot\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000c675",
   "metadata": {},
   "source": [
    "# Load In Processed Class Balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c7d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from outputs_processing/processed_data.pkl\n",
      "Train samples: 9912, Test samples: 4248\n",
      "Example text: complexity course concentrated last week...\n"
     ]
    }
   ],
   "source": [
    "# load the processed data \n",
    "with open('outputs_processing/processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# unpack everything you saved\n",
    "X_train_tfidf = data['X_train_tfidf']\n",
    "X_test_tfidf  = data['X_test_tfidf']\n",
    "y_train_enc   = data['y_train_enc']\n",
    "y_test_enc    = data['y_test_enc']\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test  = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test  = data['y_test']\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print(\"Data loaded successfully from outputs_processing/processed_data.pkl\")\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Example text: {X_train[0][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d211d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "processed",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "abe7edf5-de15-416e-9539-4e02baaaed36",
       "rows": [
        [
         "8405",
         "much disjointed information felt absolutely crushed trying learn understand waiting another hour reattempt quiz personally feel course assumes student automatically expert statistic simply due completing first intro statistic course logical progression approach different problem terminology statement involved thrown window new statistic suggest least double time allocation provided"
        ],
        [
         "6444",
         "pain installing necessary tool course linux made little difficult keep"
        ],
        [
         "12444",
         "faculty good explanation thank much coursera michigan university"
        ],
        [
         "10962",
         "would let leave course signing due family issue could continue due weird policy purchasing class could drop still"
        ],
        [
         "13863",
         "course change way thinking thank lot"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "8405     much disjointed information felt absolutely cr...\n",
       "6444     pain installing necessary tool course linux ma...\n",
       "12444    faculty good explanation thank much coursera m...\n",
       "10962    would let leave course signing due family issu...\n",
       "13863                 course change way thinking thank lot\n",
       "Name: processed, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0858524",
   "metadata": {},
   "source": [
    "# Define Encoding for Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "681454c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6224\n"
     ]
    }
   ],
   "source": [
    "# 1. Build vocabulary from training text\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for txt in texts:\n",
    "        counter.update(txt.split())\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(X_train)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# 2. Encoding function\n",
    "def encode_text(text, vocab, max_len=100):\n",
    "    tokens = text.split()\n",
    "    ids = [vocab.get(tok, vocab['<UNK>']) for tok in tokens[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab['<PAD>']] * (max_len - len(ids))\n",
    "    return torch.tensor(ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ebebf",
   "metadata": {},
   "source": [
    "# Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7da29205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_tensor = encode_text(self.texts[idx], self.vocab, self.max_len)\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return text_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4492dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for text and labels.\n",
    "    Works whether texts/labels are lists or numpy arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, transform=None):\n",
    "        # Convert to lists to ensure standard indexing\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11eb881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train_enc)\n",
    "test_dataset  = TextDataset(X_test, y_test_enc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8cdb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even though course cover lot fundamental introduction algorithm course designed good expected starte\n",
      "tensor([0, 2, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for texts, labels in train_loader:\n",
    "    print(texts[0][:100])\n",
    "    print(labels[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "729f16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenizedTextDataset(X_train, y_train_enc, vocab, max_len=100)\n",
    "test_dataset  = TokenizedTextDataset(X_test, y_test_enc, vocab, max_len=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d40a45",
   "metadata": {},
   "source": [
    "# Define Baseline LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01227daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4b4f7",
   "metadata": {},
   "source": [
    "# Define Baseline GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "947ffcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1ddad",
   "metadata": {},
   "source": [
    "# Define Training Loop CPU Friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e68e0818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.0972\n",
      "Epoch 2/3, Loss: 1.0899\n",
      "Epoch 3/3, Loss: 1.0813\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  \n",
    "\n",
    "model = LSTMClassifier(vocab_size=vocab_size, num_classes=len(set(y_train_enc)))\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7259e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.16%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        preds = model(batch_X)\n",
    "        predicted = torch.argmax(preds, dim=1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4235691",
   "metadata": {},
   "source": [
    "# Visualize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "588c070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LSTMClassifier                           [3]                       --\n",
       "├─Embedding: 1-1                         [100, 128]                796,672\n",
       "├─LSTM: 1-2                              [100, 128]                132,096\n",
       "├─Linear: 1-3                            [3]                       387\n",
       "==========================================================================================\n",
       "Total params: 929,155\n",
       "Trainable params: 929,155\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.77\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.20\n",
       "Params size (MB): 3.72\n",
       "Estimated Total Size (MB): 3.92\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMClassifier(vocab_size=vocab_size, num_classes=len(set(y_train_enc)))\n",
    "\n",
    "summary(model, input_size=(100,), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4164ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_architecture.png'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, vocab_size, (1, 100))  # dummy input\n",
    "y = model(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('model_architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "743de51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(process:23512): Pango-WARNING **: 20:41:07.491: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lstm_architecture.png'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, vocab_size, (1, 100))\n",
    "model_graph = draw_graph(model, input_data=x, expand_nested=True)\n",
    "model_graph.visual_graph.render(\"lstm_architecture\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_ENV_ITRPA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
