{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LmRvX5VoPj6"
   },
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41699,
     "status": "ok",
     "timestamp": 1761593083962,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "0h4aVY81u8S8",
    "outputId": "3bbeb9a1-4a34-4bd6-f481-57f92cc5723e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1761593084166,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "WuxKm4suq6-z",
    "outputId": "e202d081-e9ba-4923-c092-17a38c0280ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 27 19:24:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   44C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761593084184,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "zBQAftTiq8WH",
    "outputId": "469c3fbd-91fe-469b-9738-45b4428b3316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 56.9 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFC_Fcainhai"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27389,
     "status": "ok",
     "timestamp": 1761593111574,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "62KLuYS3nicj",
    "outputId": "f629eb76-0714-4d9a-be29-3d81b79c505f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchview\n",
      "  Downloading torchview-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchview) (0.21)\n",
      "Downloading torchview-0.2.7-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: torchview\n",
      "Successfully installed torchview-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install torchview\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, Trainer, TrainingArguments, AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "from torchview import draw_graph\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPWPhwlEnky4"
   },
   "source": [
    "# Setting up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111595,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "wEj8YorEreMs"
   },
   "outputs": [],
   "source": [
    "def setup_paths(model_name: str, base_dir=\"/content/drive/MyDrive/ITRPA_PROJ\"):\n",
    "    \"\"\"\n",
    "    Initialize directory structure for a given model.\n",
    "    Returns base_dir, output_dir, model_dir.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(base_dir, \"outputs_colab_final\")\n",
    "    model_dir = os.path.join(output_dir, model_name)\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Base directory: {base_dir}\")\n",
    "    print(f\"Outputs directory: {output_dir}\")\n",
    "    print(f\"Model directory: {model_dir}\")\n",
    "\n",
    "    return base_dir, output_dir, model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzJM_F6Gr4Um"
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111597,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "9HXwetl2r5jv"
   },
   "outputs": [],
   "source": [
    "def load_datafile(config, data_filename=\"reviews.csv\", use_cached=True):\n",
    "    \"\"\"\n",
    "    Loads or reloads the dataset depending on 'use_cached' flag.\n",
    "\n",
    "    If use_cached=True:\n",
    "        - Tries to find dataset already loaded in memory (globals()).\n",
    "        - Otherwise loads from CSV and constructs X, y, label_enc.\n",
    "\n",
    "    If use_cached=False:\n",
    "        - Always reloads dataset from disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing BASE_DIR or DATA_DIR keys.\n",
    "    data_filename : str, optional\n",
    "        CSV filename (default: 'reviews.csv').\n",
    "    use_cached : bool, optional\n",
    "        If True, reuse dataset if already loaded in memory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.Series\n",
    "        Text reviews\n",
    "    y_encoded : np.ndarray\n",
    "        Encoded sentiment labels (0=negative, 1=neutral, 2=positive)\n",
    "    label_enc : LabelEncoder\n",
    "        Fitted encoder mapping sentiments to integers\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Check if data already exists in memory\n",
    "    if use_cached:\n",
    "        if all(var in globals() for var in [\"X\", \"y\", \"label_enc\"]):\n",
    "            print(\"Dataset found in memory — using cached variables.\")\n",
    "            return globals()[\"X\"], globals()[\"y\"], globals()[\"label_enc\"]\n",
    "        else:\n",
    "            print(\"No cached dataset found. Reloading from disk...\")\n",
    "\n",
    "    # --- Step 2: Load dataset from disk\n",
    "    base_dir = config.get(\"BASE_DIR\", \"/content/drive/MyDrive/ITRPA_PROJ\")\n",
    "    data_dir = os.path.join(base_dir, \"data\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    data_path = os.path.join(data_dir, data_filename)\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {data_path}\")\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    if not {\"Review\", \"Label\"}.issubset(df.columns):\n",
    "        raise ValueError(\"CSV must contain 'Review' and 'Label' columns.\")\n",
    "\n",
    "    # --- Step 3: Process dataset\n",
    "    X = df[\"Review\"].astype(str)\n",
    "    y = pd.to_numeric(df[\"Label\"], errors=\"coerce\")\n",
    "\n",
    "    if not y.dropna().between(1, 5).all():\n",
    "        raise ValueError(\"Label column must contain numeric values 1–5.\")\n",
    "\n",
    "    mapping = {1: \"negative\", 2: \"negative\", 3: \"neutral\", 4: \"positive\", 5: \"positive\"}\n",
    "    y_sentiment = y.map(mapping)\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    y_encoded = label_enc.fit_transform(y_sentiment)\n",
    "\n",
    "    # --- Step 4: Store in globals for reuse\n",
    "    globals()[\"X\"], globals()[\"y\"], globals()[\"label_enc\"] = X, y_encoded, label_enc\n",
    "\n",
    "    # --- Step 5: Summary output\n",
    "    print(f\"Loaded '{data_filename}' from {data_dir} | Shape: {df.shape}\")\n",
    "    print(\"Sentiment distribution:\")\n",
    "    print(y_sentiment.value_counts())\n",
    "    print(\"Label encoding:\", dict(zip(label_enc.classes_, label_enc.transform(label_enc.classes_))))\n",
    "\n",
    "    return X, y_encoded, label_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edvBYzY3vAzc"
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111599,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "yIs8OwWdvDj8"
   },
   "outputs": [],
   "source": [
    "def prepare_splits(X, y, train_size=0.7, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits X and y into train, validation, and test sets,\n",
    "    then fits and applies a LabelEncoder consistently across all.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like or DataFrame\n",
    "        Input features.\n",
    "    y : array-like or Series\n",
    "        Target labels.\n",
    "    train_size : float, default=0.7\n",
    "        Proportion of data to use for training.\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_val, X_test : arrays/DataFrames\n",
    "        Feature splits.\n",
    "    y_train_enc, y_val_enc, y_test_enc : arrays\n",
    "        Encoded label splits.\n",
    "    label_enc : LabelEncoder\n",
    "        Fitted LabelEncoder instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # First split train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y,\n",
    "        train_size=train_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    # Split temp equally into val/test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.5,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Label encode consistently across all sets\n",
    "    label_enc = LabelEncoder()\n",
    "    label_enc.fit(list(y_train) + list(y_val) + list(y_test))\n",
    "\n",
    "    y_train_enc = label_enc.transform(y_train)\n",
    "    y_val_enc   = label_enc.transform(y_val)\n",
    "    y_test_enc  = label_enc.transform(y_test)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\" Train: {len(X_train)/len(X):.1%}\")\n",
    "    print(f\" Val:   {len(X_val)/len(X):.1%}\")\n",
    "    print(f\" Test:  {len(X_test)/len(X):.1%}\")\n",
    "    print(\"Classes found:\", label_enc.classes_)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train_enc, y_val_enc, y_test_enc, label_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D58gF8BVC8Nb"
   },
   "source": [
    "# Setting up Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111601,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "FlV0-OGoDAXU"
   },
   "outputs": [],
   "source": [
    "def load_base_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    base_model = AutoModel.from_pretrained(model_name)\n",
    "    print(f\"Loaded base model: {model_name}\")\n",
    "    return tokenizer, base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pOz9g-7DoQz"
   },
   "source": [
    "# Setting Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111602,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "9FK6aAr6DuSd"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=128, inference=False):\n",
    "        \"\"\"\n",
    "        texts: list/array of strings\n",
    "        labels: list/array of ints (optional)\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        max_len: maximum sequence length\n",
    "        inference: bool, if True, skip labels entirely (for inference)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.inference = inference\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "        # Only attach labels if not inference mode\n",
    "        if not self.inference:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wrehL9yNYSP"
   },
   "source": [
    "# Create Optimizer and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761593111612,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "m48IJMnzNcB6"
   },
   "outputs": [],
   "source": [
    "def build_optimizer(\n",
    "    model,\n",
    "    optimizer_name=\"adamw\",\n",
    "    lr_base=2e-5,\n",
    "    lr_lstm=1e-4,\n",
    "    lr_classifier=1e-4,\n",
    "    weight_decay_base=0.01,\n",
    "    weight_decay_others=0.0,\n",
    "    fine_tune_base=True  #  allow the config to control this\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an optimizer with explicit learning rates for:\n",
    "      - Transformer base (BERT/Roberta/etc.)\n",
    "      - Recurrent block (RNN/LSTM/GRU)\n",
    "      - Classifier head\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model with attributes like .bert, .roberta, .base_model, .rnn/.gru/.lstm, and .classifier\n",
    "        optimizer_name (str): 'adamw', 'adam', or 'sgd'\n",
    "        lr_base (float): learning rate for transformer/base model\n",
    "        lr_lstm (float): learning rate for RNN/LSTM/GRU\n",
    "        lr_classifier (float): learning rate for classifier head\n",
    "        weight_decay_base (float): weight decay for base model\n",
    "        weight_decay_others (float): weight decay for RNN/classifier\n",
    "        fine_tune_base (bool): if False, excludes the base transformer from optimizer groups\n",
    "    \"\"\"\n",
    "    name = optimizer_name.lower()\n",
    "    opt_classes = {\n",
    "        \"adamw\": torch.optim.AdamW,\n",
    "        \"adam\": torch.optim.Adam,\n",
    "        \"sgd\": torch.optim.SGD,\n",
    "    }\n",
    "    if name not in opt_classes:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Identify base transformer model\n",
    "    base_model = None\n",
    "    for attr in [\"bert\", \"roberta\", \"distilbert\", \"base_model\"]:\n",
    "        if hasattr(model, attr):\n",
    "            base_model = getattr(model, attr)\n",
    "            break\n",
    "\n",
    "    # Only include base params if fine_tune_base=True\n",
    "    base_params = []\n",
    "    if fine_tune_base and base_model is not None:\n",
    "        base_params = list(base_model.parameters())\n",
    "\n",
    "    # Identify recurrent layer\n",
    "    rnn_params = []\n",
    "    for attr in [\"rnn\", \"gru\", \"bigru\", \"lstm\"]:\n",
    "        if hasattr(model, attr):\n",
    "            rnn_params = list(getattr(model, attr).parameters())\n",
    "            break\n",
    "\n",
    "    # Identify classifier head\n",
    "    cls_params = list(model.classifier.parameters()) if hasattr(model, \"classifier\") else []\n",
    "\n",
    "    # Build parameter groups\n",
    "    param_groups = []\n",
    "    if base_params:\n",
    "        param_groups.append({\n",
    "            \"params\": base_params,\n",
    "            \"lr\": lr_base,\n",
    "            \"weight_decay\": weight_decay_base\n",
    "        })\n",
    "    if rnn_params:\n",
    "        param_groups.append({\n",
    "            \"params\": rnn_params,\n",
    "            \"lr\": lr_lstm,\n",
    "            \"weight_decay\": weight_decay_others\n",
    "        })\n",
    "    if cls_params:\n",
    "        param_groups.append({\n",
    "            \"params\": cls_params,\n",
    "            \"lr\": lr_classifier,\n",
    "            \"weight_decay\": weight_decay_others\n",
    "        })\n",
    "\n",
    "    if not param_groups:\n",
    "        param_groups = [{\n",
    "            \"params\": model.parameters(),\n",
    "            \"lr\": lr_base,\n",
    "            \"weight_decay\": weight_decay_base\n",
    "        }]\n",
    "        print(\"No specific groups found — using all model parameters as one group.\")\n",
    "\n",
    "    # Create optimizer\n",
    "    if name == \"sgd\":\n",
    "        optimizer = opt_classes[name](param_groups, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = opt_classes[name](param_groups)\n",
    "\n",
    "    # Summary\n",
    "    print(\"Optimizer parameter groups:\")\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        n_params = sum(p.numel() for p in g[\"params\"])\n",
    "        print(f\"  Group {i}: lr={g['lr']:.1e}, weight_decay={g['weight_decay']}, n_params={n_params}\")\n",
    "\n",
    "    if not fine_tune_base:\n",
    "        print(\"Note: fine_tune_base=False → Transformer backbone excluded from training.\")\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111614,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "3_1QqkBMNf9g"
   },
   "outputs": [],
   "source": [
    "def build_loss(loss_name, class_weights=None):\n",
    "    loss_name = loss_name.lower()\n",
    "\n",
    "    if loss_name == \"crossentropy\":\n",
    "        if class_weights is not None:\n",
    "            return nn.CrossEntropyLoss(weight=class_weights)\n",
    "        return nn.CrossEntropyLoss()\n",
    "\n",
    "    elif loss_name == \"bce\":\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    elif loss_name == \"mse\":\n",
    "        return nn.MSELoss()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss: {loss_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111617,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "zym5KN-yNoV6"
   },
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train, device):\n",
    "    \"\"\"\n",
    "    Compute balanced class weights for CrossEntropyLoss.\n",
    "\n",
    "    Args:\n",
    "        y_train: array-like of encoded class labels (e.g., [0, 1, 2, 2, 0, ...])\n",
    "        device: torch.device to move the tensor to (e.g., 'cuda' or 'cpu')\n",
    "\n",
    "    Returns:\n",
    "        class_weights: torch.Tensor of shape [num_classes]\n",
    "        weights_dict:  dict mapping class_id -> weight (for logging)\n",
    "    \"\"\"\n",
    "    y_train = np.array(y_train)\n",
    "    classes = np.unique(y_train)\n",
    "\n",
    "    weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,\n",
    "        y=y_train\n",
    "    )\n",
    "\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "    weights_dict = dict(zip(classes.tolist(), weights.tolist()))\n",
    "\n",
    "    print(\"Computed class weights:\", weights_dict)\n",
    "    return class_weights, weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BniMIUirj57r"
   },
   "source": [
    "# Create Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593111629,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "SCtrCfMZj8Uk"
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train=None, y_train=None,\n",
    "                       X_val=None, y_val=None,\n",
    "                       X_test=None, y_test=None,\n",
    "                       tokenizer=None, max_len=128, batch_size=16,\n",
    "                       inference_texts=None,  # NEW\n",
    "                       balance=True):\n",
    "    \"\"\"\n",
    "    Creates DataLoaders for training, validation, testing, and optional inference.\n",
    "    If `inference_texts` is provided, returns an inference DataLoader only.\n",
    "    \"\"\"\n",
    "\n",
    "    def make_loader(X, y=None, shuffle=False, balance=False):\n",
    "        if y is not None:\n",
    "            # --- Training or validation dataset (with labels) ---\n",
    "            dataset = TextDataset(\n",
    "                texts=X.tolist() if hasattr(X, \"tolist\") else list(X),\n",
    "                labels=y.tolist() if hasattr(y, \"tolist\") else list(y),\n",
    "                tokenizer=tokenizer,\n",
    "                max_len=max_len\n",
    "            )\n",
    "        else:\n",
    "            # --- Inference dataset (no labels) ---\n",
    "            dataset = TextDataset(\n",
    "                texts=X.tolist() if hasattr(X, \"tolist\") else list(X),\n",
    "                tokenizer=tokenizer,\n",
    "                max_len=max_len\n",
    "            )\n",
    "\n",
    "        if balance and y is not None:\n",
    "            labels = np.array(y)\n",
    "            class_counts = np.bincount(labels)\n",
    "            class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "            sample_weights = class_weights[labels]\n",
    "            sampler = WeightedRandomSampler(\n",
    "                weights=torch.DoubleTensor(sample_weights),\n",
    "                num_samples=len(sample_weights),\n",
    "                replacement=True\n",
    "            )\n",
    "            return DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "        else:\n",
    "            return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # --- Handle inference-only mode ---\n",
    "    if inference_texts is not None:\n",
    "        infer_loader = make_loader(inference_texts, y=None, shuffle=False, balance=False)\n",
    "        print(f\"Inference DataLoader ready — {len(infer_loader.dataset)} samples.\")\n",
    "        return infer_loader\n",
    "\n",
    "    # --- Normal supervised mode ---\n",
    "    train_loader = make_loader(X_train, y_train, shuffle=True, balance=balance)\n",
    "    train_eval_loader = make_loader(X_train, y_train, shuffle=False, balance=False)\n",
    "    val_loader = make_loader(X_val, y_val, shuffle=False)\n",
    "    test_loader = make_loader(X_test, y_test, shuffle=False)\n",
    "\n",
    "    print(f\"Dataloaders ready — Train: {len(train_loader.dataset)}, \"\n",
    "          f\"Val: {len(val_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
    "\n",
    "    return train_loader, train_eval_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvgBpxHmNJQz"
   },
   "source": [
    "# Create Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761598897722,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "YOjfVL2Wicxt"
   },
   "outputs": [],
   "source": [
    "def build_model(base_model, config, device):\n",
    "    name = config.get(\"model_name\", \"\").lower()\n",
    "    hidden_size = config.get(\"hidden_size\", 320)\n",
    "    dropout = config.get(\"dropout\", 0.5)\n",
    "    activation = config.get(\"activation\", \"relu\")\n",
    "    fine_tune = config.get(\"fine_tune_base\", True)\n",
    "    num_classes = config.get(\"num_classes\", 3)\n",
    "    input_size = base_model.config.hidden_size\n",
    "\n",
    "    # --- Select architecture ---\n",
    "    if \"bilstm\" in name:\n",
    "        model = TransformerBiLSTMClassifier(\n",
    "            base_model=base_model,\n",
    "            num_classes=num_classes,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            fine_tune_base=fine_tune\n",
    "        )\n",
    "\n",
    "    elif \"lstm\" in name:\n",
    "        model = TransformerLSTMClassifier(\n",
    "            base_model=base_model,\n",
    "            num_classes=num_classes,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            fine_tune_base=fine_tune\n",
    "        )\n",
    "\n",
    "    elif \"bigru\" in name:\n",
    "        model = TransformerGRUClassifier(\n",
    "            base_model=base_model,\n",
    "            num_classes=num_classes,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            fine_tune_base=fine_tune,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "\n",
    "    elif \"gru\" in name:\n",
    "        model = TransformerGRUClassifier(\n",
    "            base_model=base_model,\n",
    "            num_classes=num_classes,\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            fine_tune_base=fine_tune\n",
    "        )\n",
    "\n",
    "    elif \"cnn\" in name:\n",
    "        model = TransformerCNNClassifier(\n",
    "            base_model=base_model,\n",
    "            num_classes=num_classes,\n",
    "            input_size=input_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            fine_tune_base=fine_tune\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Default transformer classification head\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config[\"base_model_name\"], num_labels=num_classes\n",
    "        )\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761593111669,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "8CohObl6NSGo"
   },
   "outputs": [],
   "source": [
    "class TransformerLSTMClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=3, input_size=768,\n",
    "                 hidden_size=320, dropout=0.5, activation=\"relu\",\n",
    "                 fine_tune_base=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Optionally freeze transformer layers\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = fine_tune_base\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dynamic activation lookup\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leakyrelu\": nn.LeakyReLU()\n",
    "        }\n",
    "        act = activations.get(activation.lower(), nn.ReLU())\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 80),\n",
    "            act,\n",
    "            nn.Linear(80, 20),\n",
    "            act,\n",
    "            nn.Linear(20, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: dict from tokenizer (input_ids, attention_mask)\n",
    "        returns: raw logits (not softmaxed)\n",
    "        \"\"\"\n",
    "        transformer_out = self.base_model(**inputs)\n",
    "        token_embeddings = transformer_out.last_hidden_state  # [batch, seq_len, hidden]\n",
    "        lstm_out, _ = self.lstm(token_embeddings)\n",
    "        last_hidden = lstm_out[:, -1, :]  # final timestep\n",
    "        logits = self.classifier(last_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761593111694,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "6mTXeqIzfZQQ"
   },
   "outputs": [],
   "source": [
    "class TransformerBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=3, input_size=768,\n",
    "                 hidden_size=320, dropout=0.5, activation=\"relu\",\n",
    "                 fine_tune_base=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Optionally freeze transformer layers\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = fine_tune_base\n",
    "\n",
    "        # BiLSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True     # <- key change\n",
    "        )\n",
    "\n",
    "        # Dynamic activation lookup\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leakyrelu\": nn.LeakyReLU()\n",
    "        }\n",
    "        act = activations.get(activation.lower(), nn.ReLU())\n",
    "\n",
    "        # Classification head (adjust input size since BiLSTM doubles hidden dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, 160),  # *2 for bidirectional\n",
    "            act,\n",
    "            nn.Linear(160, 40),\n",
    "            act,\n",
    "            nn.Linear(40, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: dict from tokenizer (input_ids, attention_mask)\n",
    "        returns: raw logits (not softmaxed)\n",
    "        \"\"\"\n",
    "        transformer_out = self.base_model(**inputs)\n",
    "        token_embeddings = transformer_out.last_hidden_state  # [batch, seq_len, hidden]\n",
    "        lstm_out, _ = self.lstm(token_embeddings)\n",
    "\n",
    "        # Concatenate last timestep from both directions\n",
    "        last_hidden = torch.cat((lstm_out[:, -1, :self.lstm.hidden_size],\n",
    "                                 lstm_out[:, 0, self.lstm.hidden_size:]), dim=1)\n",
    "\n",
    "        logits = self.classifier(last_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1761593111722,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "55-r955FHTkc"
   },
   "outputs": [],
   "source": [
    "class TransformerGRUClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes=3, input_size=768,\n",
    "                 hidden_size=320, dropout=0.5, activation=\"relu\",\n",
    "                 fine_tune_base=True, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Optionally freeze transformer layers\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = fine_tune_base\n",
    "\n",
    "        # GRU layer (with optional bidirectionality)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Activation lookup\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leakyrelu\": nn.LeakyReLU()\n",
    "        }\n",
    "        act = activations.get(activation.lower(), nn.ReLU())\n",
    "\n",
    "        # Adjust classifier input size depending on GRU directionality\n",
    "        fc_input_dim = hidden_size * (2 if bidirectional else 1)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_input_dim, 80),\n",
    "            act,\n",
    "            nn.Linear(80, 20),\n",
    "            act,\n",
    "            nn.Linear(20, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: dict from tokenizer (input_ids, attention_mask)\n",
    "        returns: raw logits (not softmaxed)\n",
    "        \"\"\"\n",
    "        transformer_out = self.base_model(**inputs)\n",
    "        token_embeddings = transformer_out.last_hidden_state  # [batch, seq_len, hidden]\n",
    "        gru_out, _ = self.gru(token_embeddings)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Concatenate last forward & backward hidden states\n",
    "            last_hidden = torch.cat(\n",
    "                (gru_out[:, -1, :self.hidden_size],\n",
    "                 gru_out[:, 0, self.hidden_size:]), dim=1\n",
    "            )\n",
    "        else:\n",
    "            # Use last timestep output for unidirectional GRU\n",
    "            last_hidden = gru_out[:, -1, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIMgn7dQmHWj"
   },
   "source": [
    "# Create Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1761593111755,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "JZhUT8R2mJKI"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, criterion, device,\n",
    "    model_name, model_dir,\n",
    "    epochs=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, logging accuracy/loss/timing and saving state_dict + environment info.\n",
    "    \"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    epoch_times = []\n",
    "\n",
    "    model.to(device)\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # ---------------- TRAIN ----------------\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\", leave=False):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device)\n",
    "            }\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # ---------------- VALIDATE ----------------\n",
    "        model.eval()\n",
    "        val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\", leave=False):\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(device)\n",
    "                }\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * labels.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc*100:.2f}%, Val Acc: {val_acc*100:.2f}% | \"\n",
    "              f\"Time: {epoch_time/60:.2f} min\")\n",
    "\n",
    "    total_time = time.time() - overall_start\n",
    "    avg_epoch_time = total_time / epochs\n",
    "\n",
    "    # ---------------- ENVIRONMENT INFO ----------------\n",
    "    gpu_info = {}\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_props = torch.cuda.get_device_properties(device)\n",
    "        gpu_info = {\n",
    "            \"gpu_name\": gpu_props.name,\n",
    "            \"total_vram_gb\": round(gpu_props.total_memory / 1e9, 2),\n",
    "            \"allocated_vram_gb\": round(torch.cuda.memory_allocated(device) / 1e9, 2),\n",
    "            \"reserved_vram_gb\": round(torch.cuda.memory_reserved(device) / 1e9, 2),\n",
    "            \"cuda_version\": torch.version.cuda\n",
    "        }\n",
    "\n",
    "    ram = psutil.virtual_memory()\n",
    "    disk = shutil.disk_usage('/')\n",
    "    env_info = {\n",
    "        \"python_version\": f\"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}\",\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"device\": str(device),\n",
    "        \"gpu_info\": gpu_info,\n",
    "        \"ram_used_gb\": round((ram.total - ram.available) / 1e9, 2),\n",
    "        \"ram_total_gb\": round(ram.total / 1e9, 2),\n",
    "        \"disk_used_gb\": round(disk.used / 1e9, 2),\n",
    "        \"disk_total_gb\": round(disk.total / 1e9, 2)\n",
    "    }\n",
    "\n",
    "    # ---------------- LOG TIMING + ENV ----------------\n",
    "    timing_log = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"total_time_sec\": round(total_time, 2),\n",
    "        \"total_time_min\": round(total_time / 60, 2),\n",
    "        \"avg_epoch_time_sec\": round(avg_epoch_time, 2),\n",
    "        \"epoch_times_sec\": [round(t, 2) for t in epoch_times],\n",
    "        \"environment\": env_info\n",
    "    }\n",
    "\n",
    "    log_path = os.path.join(model_dir, f\"{model_name}_timing_log.json\")\n",
    "    with open(log_path, \"w\") as f:\n",
    "        json.dump(timing_log, f, indent=4)\n",
    "\n",
    "    print(f\"\\nTraining complete in {total_time/60:.2f} min \"\n",
    "          f\"({avg_epoch_time:.2f} sec/epoch avg)\")\n",
    "    print(f\"Timing and environment info saved to: {log_path}\")\n",
    "\n",
    "    # ---------------- SAVE STATE DICTIONARY ONLY ----------------\n",
    "    model_state_path = os.path.join(model_dir, f\"{model_name}_state_dict.pt\")\n",
    "    torch.save(model.state_dict(), model_state_path)\n",
    "    print(f\"Model state_dict saved to: {model_state_path}\")\n",
    "\n",
    "    # ---------------- PLOTS ----------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label=\"Val Loss\", marker='o')\n",
    "    plt.title(f\"{model_name} Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(model_dir, f\"{model_name}_training_loss.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_accs, label=\"Train Accuracy\", marker='o')\n",
    "    plt.plot(range(1, epochs + 1), val_accs, label=\"Val Accuracy\", marker='o')\n",
    "    plt.title(f\"{model_name} Training Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(model_dir, f\"{model_name}_training_accuracy.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"timing_log\": timing_log,\n",
    "        \"model_paths\": {\n",
    "            \"state_dict\": model_state_path\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMp32KgBpVeE"
   },
   "source": [
    "# Model Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1761593111781,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "YFVTh_vwpXgo"
   },
   "outputs": [],
   "source": [
    "def save_model_graph(model, dataloader, model_name, model_dir, device, show_backbone=True):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))\n",
    "    inputs = {\n",
    "        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"attention_mask\"].to(device)\n",
    "    }\n",
    "\n",
    "    if not show_backbone:\n",
    "        class DummyBase(nn.Module):\n",
    "            def __init__(self, hidden_size=768):\n",
    "                super().__init__()\n",
    "                self.config = type('obj', (object,), {\"hidden_size\": hidden_size})\n",
    "\n",
    "            def forward(self, **kwargs):\n",
    "                batch_size = kwargs[\"input_ids\"].shape[0]\n",
    "                seq_len = kwargs[\"input_ids\"].shape[1]\n",
    "                hidden_size = self.config.hidden_size\n",
    "                return type('obj', (object,), {\n",
    "                    \"last_hidden_state\": torch.randn(\n",
    "                        batch_size, seq_len, hidden_size,\n",
    "                        device=kwargs[\"input_ids\"].device\n",
    "                    )\n",
    "                })\n",
    "\n",
    "        model.base_model = DummyBase()\n",
    "\n",
    "    class TorchViewWrapper(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "        def forward(self, inputs):\n",
    "            return self.model(inputs)\n",
    "\n",
    "    wrapped_model = TorchViewWrapper(model).to(device)\n",
    "    wrapped_model.eval()\n",
    "\n",
    "    graph = draw_graph(\n",
    "        wrapped_model,\n",
    "        input_data=(inputs,),\n",
    "        expand_nested=True,\n",
    "        depth=4,\n",
    "        device=device,\n",
    "        save_graph=True,\n",
    "        directory=model_dir,\n",
    "        filename=f\"{model_name.replace('/', '_')}_graph\"\n",
    "    )\n",
    "\n",
    "    print(f\"Saved model graph to: {model_dir}/{model_name.replace('/', '_')}_graph.png\")\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Udl8-KuJr_pV"
   },
   "source": [
    "# Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1761593111914,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "6WNHwAmC8Gzv"
   },
   "outputs": [],
   "source": [
    "def make_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert numpy data types and other non-serializable\n",
    "    values (e.g., np.int64, np.float32) into JSON-safe types.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): make_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_json_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1761593112325,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "U1dPNTHasBhN"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    label_encoder,\n",
    "    device,\n",
    "    model_name,\n",
    "    model_dir,\n",
    "    num_classes=3,\n",
    "    train_acc_final=None,\n",
    "    config=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the test set and visualize key metrics.\n",
    "\n",
    "    Saves:\n",
    "        - classification_report.json (with class names)\n",
    "        - roc_auc.png (with label names)\n",
    "        - predictions.pkl (includes decoded labels)\n",
    "        - confusion_matrix.png, accuracy_barplot.png (optional)\n",
    "    \"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # ---------------- Save CONFIG ----------------\n",
    "    if config is not None:\n",
    "        config_path = os.path.join(model_dir, f\"{model_name}_config.json\")\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f, indent=4, default=str)\n",
    "        print(f\"Config saved → {config_path}\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name}\", leave=False):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device)\n",
    "            }\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    preds = np.array(all_preds)\n",
    "    probs = np.array(all_probs)\n",
    "    labels = np.array(all_labels)\n",
    "\n",
    "    # ---------------- Decode to class names ----------------\n",
    "    class_names = label_encoder.classes_       # e.g. ['negative', 'neutral', 'positive']\n",
    "    y_true = label_encoder.inverse_transform(labels)\n",
    "    y_pred = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    # ---------------- Save raw predictions ----------------\n",
    "    pred_dict = {\n",
    "        \"y_true_idx\": labels.tolist(),\n",
    "        \"y_pred_idx\": preds.tolist(),\n",
    "        \"y_true\": y_true.tolist(),\n",
    "        \"y_pred\": y_pred.tolist(),\n",
    "        \"probs\": probs.tolist(),\n",
    "        \"class_names\": class_names.tolist()\n",
    "    }\n",
    "    preds_path = os.path.join(model_dir, f\"{model_name}_predictions.pkl\")\n",
    "    with open(preds_path, \"wb\") as f:\n",
    "        pickle.dump(pred_dict, f)\n",
    "    print(f\"Predictions saved → {preds_path}\")\n",
    "\n",
    "    # ---------------- Classification report ----------------\n",
    "    report_path = os.path.join(model_dir, f\"{model_name}_classification_report.json\")\n",
    "\n",
    "    report = classification_report(\n",
    "    y_true, y_pred, target_names=class_names,\n",
    "    output_dict=True, zero_division=0\n",
    "    )\n",
    "    report_serializable = make_json_serializable(report)\n",
    "\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report_serializable, f, indent=4)\n",
    "\n",
    "\n",
    "    # ---------------- ROC-AUC (string labels on plot) ----------------\n",
    "    # Re-binarize using integer indices for computation\n",
    "    y_true_bin = label_binarize(labels, classes=np.arange(num_classes))\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "    for i, label_name in enumerate(class_names):\n",
    "        # Handle class absence safely\n",
    "        if y_true_bin[:, i].sum() == 0:\n",
    "            fpr[i], tpr[i], roc_auc[i] = [0], [0], 0.0\n",
    "            continue\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    macro_auc = roc_auc_score(\n",
    "        y_true_bin, probs, average=\"macro\", multi_class=\"ovr\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for i, label_name in enumerate(class_names):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                 label=f\"{label_name} (AUC={roc_auc[i]:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "    plt.title(f\"{model_name} ROC Curves (Macro AUC={macro_auc:.3f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    roc_path = os.path.join(model_dir, f\"{model_name}_roc_auc.png\")\n",
    "    plt.savefig(roc_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"ROC-AUC plot saved → {roc_path}\")\n",
    "\n",
    "    # ---------------- Optional: Accuracy bar plot ----------------\n",
    "    if train_acc_final is not None:\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.bar([\"Train\", \"Test\"], [train_acc_final * 100, report[\"accuracy\"] * 100],\n",
    "                color=[\"skyblue\", \"lightcoral\"])\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title(f\"{model_name} Train vs Test Accuracy\")\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        acc_path = os.path.join(model_dir, f\"{model_name}_accuracy_barplot.png\")\n",
    "        plt.savefig(acc_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Accuracy comparison saved → {acc_path}\")\n",
    "\n",
    "    return {\n",
    "        \"report\": report,\n",
    "        \"roc_auc\": macro_auc,\n",
    "        \"predictions_file\": preds_path,\n",
    "        \"report_file\": report_path,\n",
    "        \"roc_plot\": roc_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g86nDNqsiHd"
   },
   "source": [
    "# Usage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761593112333,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "H7Z4OQekuKPY"
   },
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "    *,\n",
    "    use_cached=True,\n",
    "    make_splits=True,\n",
    "    load_model=True,\n",
    "    make_datasets=True,\n",
    "    build_opt=True,\n",
    "    build_loss_fn=True,\n",
    "    make_loaders=True,\n",
    "    init_model=True,\n",
    "    do_train=True,\n",
    "    save_graph=True,\n",
    "    do_eval=True,\n",
    "    **config\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified Deep Learning Pipeline\n",
    "\n",
    "    Execution Stages\n",
    "    ----------------\n",
    "    1. Setup directories and device\n",
    "    2. Load tokenizer and base transformer model\n",
    "    3. Load dataset (with optional caching)\n",
    "    4. Prepare train/val/test splits\n",
    "    5. Create tokenized PyTorch DataLoaders\n",
    "    6. Initialize TransformerLSTMClassifier\n",
    "    7. Build optimizer and loss function\n",
    "    8. Train the model (with logging and plots)\n",
    "    9. Save model computation graph (optional)\n",
    "    10. Evaluate on test set (classification report, ROC-AUC, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Starting pipeline setup ===\")\n",
    "\n",
    "    # -------------------- STAGE 1: Paths + Device --------------------\n",
    "    base_dir = config.get(\"BASE_DIR\", \"/content/drive/MyDrive/ITRPA_PROJ\")\n",
    "    base_model_name = config.get(\"base_model_name\", \"bert-base-uncased\")\n",
    "    model_name = config.get(\"model_name\", \"bert-bi-lstm\")\n",
    "    device = torch.device(config.get(\"device\", \"cpu\"))\n",
    "\n",
    "    model_dir = os.path.join(base_dir, \"outputs_colab_final\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # -------------------- STAGE 2: Load tokenizer + transformer base --------------------\n",
    "    tokenizer, base_model = load_base_model(base_model_name)\n",
    "    base_model.to(device)\n",
    "\n",
    "    # -------------------- STAGE 3–4: Load dataset + prepare splits --------------------\n",
    "    X, y, label_enc = load_datafile(config, data_filename=\"reviews.csv\", use_cached=use_cached)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, label_enc = prepare_splits(X, y)\n",
    "\n",
    "    # -------------------- STAGE 5: Create Dataloaders --------------------\n",
    "    train_loader, train_eval_loader, val_loader, test_loader = create_dataloaders(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        tokenizer, config[\"max_len\"], config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # -------------------- STAGE 6: Initialize model --------------------\n",
    "    model = build_model(base_model, config, device)\n",
    "\n",
    "    # -------------------- STAGE 7: Build optimizer + loss --------------------\n",
    "    optimizer = build_optimizer(\n",
    "        model,\n",
    "        optimizer_name=config.get(\"optimizer\", \"adamw\"),\n",
    "        lr_base=config.get(\"lr_base\", 2e-5),\n",
    "        lr_lstm=config.get(\"lr_lstm\", 1e-4),\n",
    "        lr_classifier=config.get(\"lr_classifier\", 1e-4),\n",
    "        weight_decay_base=config.get(\"weight_decay_base\", 0.01),\n",
    "        weight_decay_others=config.get(\"weight_decay_others\", 0.0),\n",
    "        fine_tune_base=config.get(\"fine_tune_base\", True)\n",
    "    )\n",
    "\n",
    "\n",
    "    loss_fn = build_loss(config[\"loss_fn\"])\n",
    "\n",
    "    # -------------------- STAGE 8: Train model --------------------\n",
    "    train_acc_final = None\n",
    "    if do_train:\n",
    "        print(\"\\n=== Beginning model training ===\")\n",
    "        train_log = train_and_evaluate(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=loss_fn,\n",
    "            device=device,\n",
    "            model_name=model_name,\n",
    "            model_dir=model_dir,\n",
    "            epochs=config[\"epochs\"]\n",
    "        )\n",
    "        train_acc_final = train_log[\"train_accs\"][-1] if len(train_log[\"train_accs\"]) > 0 else None\n",
    "    else:\n",
    "        print(\"Skipping training (do_train=False)\")\n",
    "\n",
    "    # -------------------- STAGE 9: Save model graph --------------------\n",
    "    if save_graph:\n",
    "        print(\"\\n=== Saving model graph ===\")\n",
    "        save_model_graph(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            model_name=model_name,\n",
    "            model_dir=model_dir,\n",
    "            device=device,\n",
    "            show_backbone=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping graph generation (save_graph=False)\")\n",
    "\n",
    "    # -------------------- STAGE 10: Evaluate on test set --------------------\n",
    "    if do_eval:\n",
    "        print(\"\\n=== Evaluating model on test set ===\")\n",
    "        results = evaluate_model(\n",
    "            model=model,\n",
    "            test_loader=test_loader,\n",
    "            label_encoder=label_enc,\n",
    "            device=device,\n",
    "            model_name=model_name,\n",
    "            model_dir=model_dir,\n",
    "            num_classes=config.get(\"num_classes\", 3),\n",
    "            train_acc_final=train_acc_final,\n",
    "            config=config\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping evaluation (do_eval=False)\")\n",
    "        results = None\n",
    "\n",
    "    print(\"=== Pipeline complete ===\")\n",
    "\n",
    "    # -------------------- Return outputs --------------------\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"label_enc\": label_enc,\n",
    "        \"dataloaders\": {\n",
    "            \"train\": train_loader,\n",
    "            \"val\": val_loader,\n",
    "            \"test\": test_loader\n",
    "        },\n",
    "        \"results\": results,\n",
    "        \"config\": config\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr1h7LOUyTeE"
   },
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1761593112338,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "HUlUc49hyUpr"
   },
   "outputs": [],
   "source": [
    "# CONFIG = {\n",
    "#     # -------- Model & Architecture --------\n",
    "#     \"model_name\": \"roberta-lstm-gelu-lr-adj\",\n",
    "#     \"base_model_name\": \"roberta-base\",\n",
    "#     \"activation\": \"gelu\",\n",
    "#     \"hidden_size\": 320,\n",
    "#     \"dropout\": 0.5,\n",
    "#     \"fine_tune_base\": True,\n",
    "#     \"num_classes\": 3,\n",
    "\n",
    "#     # -------- Tokenization & Data --------\n",
    "#     \"max_len\": 160,\n",
    "#     \"batch_size\": 128,\n",
    "\n",
    "#     # -------- Training --------\n",
    "#     \"epochs\": 2,\n",
    "#     \"optimizer\": \"adamw\",\n",
    "#     \"loss_fn\": \"crossentropy\",\n",
    "\n",
    "#     # -------- Learning Rates --------\n",
    "#     \"lr_base\": 2e-05,\n",
    "#     \"lr_lstm\": 1e-04,\n",
    "#     \"lr_classifier\": 1e-03,\n",
    "\n",
    "#     # -------- Weight Decay --------\n",
    "#     \"weight_decay_base\": 0.035,\n",
    "#     \"weight_decay_others\": 0.01,\n",
    "\n",
    "#     # -------- Paths & Device --------\n",
    "#     \"BASE_DIR\": \"/content/drive/MyDrive/ITRPA_PROJ\",\n",
    "#     \"device\": str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# }\n",
    "\n",
    "\n",
    "# pipeline = run_pipeline(\n",
    "#     use_cached=True,\n",
    "#     make_splits=True,\n",
    "#     load_model=True,\n",
    "#     make_datasets=True,\n",
    "#     build_opt=True,\n",
    "#     build_loss_fn=True,\n",
    "#     make_loaders=True,\n",
    "#     init_model=True,\n",
    "#     do_train=True,\n",
    "#     save_graph=True,\n",
    "#     do_eval=True,\n",
    "#     **CONFIG\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj2ZAEjkMhPd"
   },
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761593112361,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "OjxiUmN5UKqv"
   },
   "outputs": [],
   "source": [
    "def infer_from_state_dict(\n",
    "    model_path,\n",
    "    base_model_name=\"bert-base-uncased\",\n",
    "    hidden_size=320,\n",
    "    num_classes=3,\n",
    "    activation=\"gelu\",\n",
    "    max_len=128,\n",
    "    batch_size=16,\n",
    "    fine_tune_base=True,\n",
    "    device=None,\n",
    "    texts=None,\n",
    "    csv_path=None,\n",
    "    text_column=\"review\",\n",
    "    id2label=None,\n",
    "    save_path=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform inference with a saved Transformer+LSTM classifier.\n",
    "    Supports direct CSV loading and progress bar visualization.\n",
    "\n",
    "    Returns:\n",
    "        df_results (pd.DataFrame): DataFrame with predictions and confidences\n",
    "        timing (dict): timing summary\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load data\n",
    "    if csv_path:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in {csv_path}.\")\n",
    "        texts = df[text_column].astype(str).tolist()\n",
    "        if verbose:\n",
    "            print(f\"Loaded {len(texts)} rows from {csv_path}\")\n",
    "    elif texts is None:\n",
    "        raise ValueError(\"Provide either 'texts' list or 'csv_path'.\")\n",
    "\n",
    "    # Load tokenizer and transformer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModel.from_pretrained(base_model_name)\n",
    "    input_size = base_model.config.hidden_size  # dynamic input size from base model\n",
    "\n",
    "    # Build model\n",
    "    model = TransformerLSTMClassifier(\n",
    "        base_model=base_model,\n",
    "        num_classes=num_classes,\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=0.5,\n",
    "        activation=activation,\n",
    "        fine_tune_base=fine_tune_base\n",
    "    )\n",
    "\n",
    "\n",
    "    # Load weights\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Build dataset and dataloader\n",
    "    dataset = TextDataset(texts, tokenizer=tokenizer, max_len=max_len, inference=True)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Model loaded ({base_model_name}), running on {device}\")\n",
    "        print(f\"Starting inference on {len(dataset)} samples...\")\n",
    "\n",
    "    preds, probs = [], []\n",
    "    start = time.time()\n",
    "\n",
    "    # Inference loop with progress bar\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, total=len(loader), desc=\"Inference\", leave=True):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(batch)\n",
    "            p = F.softmax(logits, dim=1)\n",
    "            preds.extend(torch.argmax(p, dim=1).cpu().numpy())\n",
    "            probs.extend(p.cpu().numpy())\n",
    "\n",
    "    total = time.time() - start\n",
    "    timing = {\n",
    "        \"total_time_sec\": total,\n",
    "        \"time_per_sample_sec\": total / len(dataset),\n",
    "        \"time_per_batch_sec\": total / len(loader)\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total time: {total:.3f}s | Per sample: {timing['time_per_sample_sec']:.4f}s\")\n",
    "\n",
    "    # Prepare results\n",
    "    df_results = pd.DataFrame({\n",
    "        \"text\": texts,\n",
    "        \"predicted_class\": preds,\n",
    "        \"confidence\": [float(np.max(p)) for p in probs]\n",
    "    })\n",
    "\n",
    "    if id2label:\n",
    "        df_results[\"label\"] = [id2label[p] for p in preds]\n",
    "\n",
    "    if csv_path:\n",
    "        df_results.insert(0, \"source_file\", os.path.basename(csv_path))\n",
    "\n",
    "    if save_path:\n",
    "        df_results.to_csv(save_path, index=False)\n",
    "        if verbose:\n",
    "            print(f\"Results saved to: {save_path}\")\n",
    "\n",
    "    return df_results, timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpt-BpYVNKve"
   },
   "source": [
    "# Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593112363,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "qd7OBHlYYQoW"
   },
   "outputs": [],
   "source": [
    "# BASE_DIR = \"/content/drive/MyDrive/ITRPA_PROJ\"\n",
    "# DATA_PATH = os.path.join(BASE_DIR, \"data\", \"reviews.csv\")\n",
    "# MODEL_PATH = os.path.join(BASE_DIR, \"outputs_colab_final\", \"roberta-lstm-gelu-lr-adj\", \"roberta-lstm-gelu-lr-adj_state_dict.pt\")\n",
    "# OUTPUT_PATH = os.path.join(BASE_DIR, \"outputs_colab_final\", \"roberta-lstm-gelu-lr-adj\",\"reviews_with_predictions.csv\")\n",
    "\n",
    "# # Label mapping (adjust as per your training)\n",
    "# id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "# # Run inference\n",
    "# df_results, timing = infer_from_state_dict(\n",
    "#     model_path=MODEL_PATH,\n",
    "#     base_model_name=\"roberta-base\",\n",
    "#     csv_path=DATA_PATH,\n",
    "#     text_column=\"Review\",   # change if your CSV uses 'text', 'comment', etc.\n",
    "#     id2label=id2label,\n",
    "#     batch_size=128,\n",
    "#     save_path=OUTPUT_PATH\n",
    "# )\n",
    "\n",
    "# print(\"Done!\")\n",
    "# print(f\"Processed {len(df_results)} rows\")\n",
    "# print(timing)\n",
    "# df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx-ut6QcyETy"
   },
   "source": [
    "# Disconnect Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1761593112364,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "5VSY8jO9yFrc"
   },
   "outputs": [],
   "source": [
    "# from google.colab import runtime\n",
    "# runtime.unassign()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m1JhXRif2yH"
   },
   "source": [
    "# BI LSTM ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761593112366,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "FKQdHAKyf4K1"
   },
   "outputs": [],
   "source": [
    "# CONFIG = {\n",
    "#     # -------- Model & Architecture --------\n",
    "#     \"model_name\": \"roberta-bilstm-gelu-lr-adj\",   # ← changed from \"lstm\" to \"bilstm\"\n",
    "#     \"base_model_name\": \"roberta-base\",\n",
    "#     \"activation\": \"gelu\",\n",
    "#     \"hidden_size\": 320,\n",
    "#     \"dropout\": 0.5,\n",
    "#     \"fine_tune_base\": True,\n",
    "#     \"num_classes\": 3,\n",
    "\n",
    "#     # -------- Tokenization & Data --------\n",
    "#     \"max_len\": 160,\n",
    "#     \"batch_size\": 128,\n",
    "\n",
    "#     # -------- Training --------\n",
    "#     \"epochs\": 4,\n",
    "#     \"optimizer\": \"adamw\",\n",
    "#     \"loss_fn\": \"crossentropy\",\n",
    "\n",
    "#     # -------- Learning Rates --------\n",
    "#     \"lr_base\": 2e-05,\n",
    "#     \"lr_lstm\": 5e-05,\n",
    "#     \"lr_classifier\": 1e-03,\n",
    "\n",
    "#     # -------- Weight Decay --------\n",
    "#     \"weight_decay_base\": 0.035,\n",
    "#     \"weight_decay_others\": 0.01,\n",
    "\n",
    "#     # -------- Paths & Device --------\n",
    "#     \"BASE_DIR\": \"/content/drive/MyDrive/ITRPA_PROJ\",\n",
    "#     \"device\": str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# }\n",
    "\n",
    "# # --- Run the pipeline ---\n",
    "# pipeline = run_pipeline(\n",
    "#     use_cached=True,\n",
    "#     make_splits=True,\n",
    "#     load_model=True,\n",
    "#     make_datasets=True,\n",
    "#     build_opt=True,\n",
    "#     build_loss_fn=True,\n",
    "#     make_loaders=True,\n",
    "#     init_model=True,\n",
    "#     do_train=True,\n",
    "#     save_graph=True,\n",
    "#     do_eval=True,\n",
    "#     **CONFIG\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtzHAImoIBEc"
   },
   "source": [
    "# ROBERTA GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4788072,
     "status": "ok",
     "timestamp": 1761597958328,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "sIAg1C16IDS-",
    "outputId": "e205b045-faa5-4319-e5c5-e1d5274d8e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting pipeline setup ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base model: roberta-base\n",
      "Dataset found in memory — using cached variables.\n",
      " Train: 70.0%\n",
      " Val:   15.0%\n",
      " Test:  15.0%\n",
      "Classes found: [0 1 2]\n",
      "Dataloaders ready — Train: 74912, Val: 16053, Test: 16053\n",
      "Optimizer parameter groups:\n",
      "  Group 0: lr=2.0e-05, weight_decay=0.035, n_params=124645632\n",
      "  Group 1: lr=1.0e-04, weight_decay=0.01, n_params=1046400\n",
      "  Group 2: lr=1.0e-03, weight_decay=0.01, n_params=27363\n",
      "\n",
      "=== Beginning model training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/5 | Train Loss: 0.4447, Val Loss: 0.3173 | Train Acc: 81.57%, Val Acc: 88.59% | Time: 15.73 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/5 | Train Loss: 0.1890, Val Loss: 0.3179 | Train Acc: 93.44%, Val Acc: 89.88% | Time: 15.73 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/5 | Train Loss: 0.1184, Val Loss: 0.3780 | Train Acc: 96.04%, Val Acc: 90.11% | Time: 15.73 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/5 | Train Loss: 0.0884, Val Loss: 0.4066 | Train Acc: 97.12%, Val Acc: 90.75% | Time: 15.72 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/5 | Train Loss: 0.0750, Val Loss: 0.3946 | Train Acc: 97.52%, Val Acc: 90.21% | Time: 15.73 min\n",
      "\n",
      "Training complete in 78.64 min (943.72 sec/epoch avg)\n",
      "Timing and environment info saved to: /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_timing_log.json\n",
      "Model state_dict saved to: /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_state_dict.pt\n",
      "\n",
      "=== Saving model graph ===\n",
      "Saved model graph to: /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_graph.png\n",
      "\n",
      "=== Evaluating model on test set ===\n",
      "Config saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_predictions.pkl\n",
      "ROC-AUC plot saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_roc_auc.png\n",
      "Accuracy comparison saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-gru-gelu-lr-adj/roberta-gru-gelu-lr-adj_accuracy_barplot.png\n",
      "=== Pipeline complete ===\n"
     ]
    }
   ],
   "source": [
    "# CONFIG = {\n",
    "#     # -------- Model & Architecture --------\n",
    "#     \"model_name\": \"roberta-gru-gelu-lr-adj\",   # ← changed from \"bilstm\" to \"gru\"\n",
    "#     \"base_model_name\": \"roberta-base\",\n",
    "#     \"activation\": \"gelu\",\n",
    "#     \"hidden_size\": 320,\n",
    "#     \"dropout\": 0.5,\n",
    "#     \"fine_tune_base\": True,\n",
    "#     \"num_classes\": 3,\n",
    "\n",
    "#     # -------- Tokenization & Data --------\n",
    "#     \"max_len\": 160,\n",
    "#     \"batch_size\": 128,\n",
    "\n",
    "#     # -------- Training --------\n",
    "#     \"epochs\": 5,\n",
    "#     \"optimizer\": \"adamw\",\n",
    "#     \"loss_fn\": \"crossentropy\",\n",
    "\n",
    "#     # -------- Learning Rates --------\n",
    "#     \"lr_base\": 2e-05,\n",
    "#     \"lr_gru\": 5e-05,          # ← renamed to match GRU layer\n",
    "#     \"lr_classifier\": 1e-03,\n",
    "\n",
    "#     # -------- Weight Decay --------\n",
    "#     \"weight_decay_base\": 0.035,\n",
    "#     \"weight_decay_others\": 0.01,\n",
    "\n",
    "#     # -------- Paths & Device --------\n",
    "#     \"BASE_DIR\": \"/content/drive/MyDrive/ITRPA_PROJ\",\n",
    "#     \"device\": str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# }\n",
    "\n",
    "# # --- Run the pipeline ---\n",
    "# pipeline = run_pipeline(\n",
    "#     use_cached=True,\n",
    "#     make_splits=True,\n",
    "#     load_model=True,\n",
    "#     make_datasets=True,\n",
    "#     build_opt=True,\n",
    "#     build_loss_fn=True,\n",
    "#     make_loaders=True,\n",
    "#     init_model=True,\n",
    "#     do_train=True,\n",
    "#     save_graph=True,\n",
    "#     do_eval=True,\n",
    "#     **CONFIG\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Opgqd98ybfE1"
   },
   "source": [
    "# ROBERTA BI GRU GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3877524,
     "status": "ok",
     "timestamp": 1761602782258,
     "user": {
      "displayName": "Alessandro G",
      "userId": "00113682379950598353"
     },
     "user_tz": -120
    },
    "id": "MLuQVqDIbg4o",
    "outputId": "b9f7a440-7c39-4f20-fa28-fbd223d6b246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting pipeline setup ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base model: roberta-base\n",
      "Dataset found in memory — using cached variables.\n",
      " Train: 70.0%\n",
      " Val:   15.0%\n",
      " Test:  15.0%\n",
      "Classes found: [0 1 2]\n",
      "Dataloaders ready — Train: 74912, Val: 16053, Test: 16053\n",
      "Optimizer parameter groups:\n",
      "  Group 0: lr=2.0e-05, weight_decay=0.035, n_params=124645632\n",
      "  Group 1: lr=1.0e-04, weight_decay=0.01, n_params=2092800\n",
      "  Group 2: lr=7.0e-04, weight_decay=0.01, n_params=52963\n",
      "\n",
      "=== Beginning model training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/4 | Train Loss: 0.4446, Val Loss: 0.3409 | Train Acc: 81.78%, Val Acc: 86.89% | Time: 15.84 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/4 | Train Loss: 0.1910, Val Loss: 0.3372 | Train Acc: 93.43%, Val Acc: 89.30% | Time: 15.83 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/4 | Train Loss: 0.1143, Val Loss: 0.3551 | Train Acc: 96.12%, Val Acc: 91.60% | Time: 15.89 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/4 | Train Loss: 0.0889, Val Loss: 0.3146 | Train Acc: 97.05%, Val Acc: 92.45% | Time: 15.90 min\n",
      "\n",
      "Training complete in 63.46 min (951.91 sec/epoch avg)\n",
      "Timing and environment info saved to: /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_timing_log.json\n",
      "Model state_dict saved to: /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_state_dict.pt\n",
      "\n",
      "=== Saving model graph ===\n",
      "Saved model graph to: /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_graph.png\n",
      "\n",
      "=== Evaluating model on test set ===\n",
      "Config saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_predictions.pkl\n",
      "ROC-AUC plot saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_roc_auc.png\n",
      "Accuracy comparison saved → /content/drive/MyDrive/ITRPA_PROJ/outputs_colab_final/roberta-bigru-gelu-lr-adj/roberta-bigru-gelu-lr-adj_accuracy_barplot.png\n",
      "=== Pipeline complete ===\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    # -------- Model & Architecture --------\n",
    "    \"model_name\": \"roberta-bigru-gelu-lr-adj\",   # changed from \"gru\" to \"bigru\"\n",
    "    \"base_model_name\": \"roberta-base\",\n",
    "    \"activation\": \"gelu\",\n",
    "    \"hidden_size\": 320,\n",
    "    \"dropout\": 0.5,\n",
    "    \"fine_tune_base\": True,\n",
    "    \"num_classes\": 3,\n",
    "\n",
    "    # -------- Tokenization & Data --------\n",
    "    \"max_len\": 160,\n",
    "    \"batch_size\": 128,\n",
    "\n",
    "    # -------- Training --------\n",
    "    \"epochs\": 4,\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"loss_fn\": \"crossentropy\",\n",
    "\n",
    "    # -------- Learning Rates --------\n",
    "    \"lr_base\": 2e-05,\n",
    "    \"lr_gru\": 4e-05,          # slightly lowered for BiGRU stability (was 5e-05)\n",
    "    \"lr_classifier\": 7e-04,   # minor reduction to balance larger BiGRU output\n",
    "\n",
    "    # -------- Weight Decay --------\n",
    "    \"weight_decay_base\": 0.035,\n",
    "    \"weight_decay_others\": 0.01,\n",
    "\n",
    "    # -------- Paths & Device --------\n",
    "    \"BASE_DIR\": \"/content/drive/MyDrive/ITRPA_PROJ\",\n",
    "    \"device\": str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "}\n",
    "\n",
    "# --- Run the pipeline ---\n",
    "pipeline = run_pipeline(\n",
    "    use_cached=True,\n",
    "    make_splits=True,\n",
    "    load_model=True,\n",
    "    make_datasets=True,\n",
    "    build_opt=True,\n",
    "    build_loss_fn=True,\n",
    "    make_loaders=True,\n",
    "    init_model=True,\n",
    "    do_train=True,\n",
    "    save_graph=True,\n",
    "    do_eval=True,\n",
    "    **CONFIG\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNtPOkf7Spcf3w0OMAY6B0W",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
